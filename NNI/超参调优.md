# 超参调优

## 1. 参数搜索空间

```
search_space = {
    'batch_size': {'_type': 'choice', '_value': [2, 4]},
    'lr': {'_type': 'uniform', '_value': [0.0001, 0.1]},
}

_type: 选择参数的策略，与numpy的random一致
  choice：随机选择。
  uniform：从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high。类型为float。
  loguniform：对数均匀  1/(log(b/a))
  
```

## 2. Tuner

```
2.1 BOHB
BOHB 是一种鲁棒有效的大规模超参数整定算法。BO 是“贝叶斯优化”的缩写，HB 是“ Hyperband”的缩写。
BOHB 依赖于 HB (Hyperband 超频)来确定用哪个预算来评估多少配置，但是它通过基于模型的搜索(贝叶斯优化)在每个 HB 迭代开始时替换配置的随机选择。一旦达到迭代所需的配置数目，就使用这些配置执行标准的连续切分过程。它跟踪所有预算 b 上配置 x 的所有功能评估 g (x，b)的性能，以作为后续迭代中模型的基础。详细的算法请参阅福克纳等人的论文1。

BOHB relies on HB (Hyperband) to determine how many configurations to evaluate with which budget, but it replaces the random selection of configurations at the beginning of each HB iteration by a model-based search (Bayesian Optimization). Once the desired number of configurations for the iteration is reached, the standard successive halving procedure is carried out using these configurations. It keeps track of the performance of all function evaluations g(x, b) of configurations x on all budgets b to use as a basis for our models in later iterations. Please refer to the paper Falkner et al.1 for detailed algorithm.

Note that BOHB needs additional installation using the following command:

pip install nni[BOHB]

2.2 TPE

pass
2.3 SMAC
```

